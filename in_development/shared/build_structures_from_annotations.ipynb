{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "from taskqueue import LocalTaskQueue\n",
    "import igneous.task_creation as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroglancer\n",
    "viewer = neuroglancer.Viewer()\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(os.path.join(HOME, 'programming', 'pipeline_utility/src'))\n",
    "from lib.FileLocationManager import FileLocationManager\n",
    "from abakit.lib.Controllers.SqlController import SqlController\n",
    "from lib.utilities_alignment import load_consecutive_section_transform, create_warp_transforms, \\\n",
    "    transform_create_alignment\n",
    "from lib.utilities_contour import get_dense_coordinates, get_contours_from_annotations\n",
    "from lib.utilities_process import SCALING_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = 'MD589'\n",
    "sqlController = SqlController(animal)\n",
    "fileLocationManager = FileLocationManager(animal)\n",
    "width = sqlController.scan_run.width\n",
    "height = sqlController.scan_run.height\n",
    "width = int(width * SCALING_FACTOR)\n",
    "height = int(height * SCALING_FACTOR)\n",
    "aligned_shape = np.array((width, height))\n",
    "THUMBNAIL_PATH = os.path.join(fileLocationManager.prep, 'CH1', 'thumbnail')\n",
    "THUMBNAILS = sorted(os.listdir(THUMBNAIL_PATH))\n",
    "num_section = len(THUMBNAILS)\n",
    "structure_dict = sqlController.get_structures_dict()\n",
    "\n",
    "\n",
    "CSV_PATH = '/net/birdstore/Active_Atlas_Data/data_root/atlas_data/foundation_brain_annotations'\n",
    "csvfile = os.path.join(CSV_PATH, f'{animal}_annotation.csv')\n",
    "hand_annotations = pd.read_csv(csvfile)\n",
    "hand_annotations['vertices'] = hand_annotations['vertices'] \\\n",
    "    .apply(lambda x: x.replace(' ', ','))\\\n",
    "    .apply(lambda x: x.replace('\\n',','))\\\n",
    "    .apply(lambda x: x.replace(',]',']'))\\\n",
    "    .apply(lambda x: x.replace(',,', ','))\\\n",
    "    .apply(lambda x: x.replace(',,', ','))\\\n",
    "    .apply(lambda x: x.replace(',,', ',')).apply(lambda x: x.replace(',,', ','))\n",
    "hand_annotations['vertices'] = hand_annotations['vertices'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "structures = list(hand_annotations['name'].unique())\n",
    "section_structure_vertices = defaultdict(dict)\n",
    "for structure in tqdm(structures):\n",
    "    contour_annotations, first_sec, last_sec = get_contours_from_annotations(animal, structure, hand_annotations, densify=4)\n",
    "    for section in contour_annotations:\n",
    "        section_structure_vertices[section][structure] = contour_annotations[section][structure][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Reproduce create_clean transform\n",
    "section_offset = {}\n",
    "for file_name in tqdm(THUMBNAILS):\n",
    "    filepath = os.path.join(THUMBNAIL_PATH, file_name)\n",
    "    img = io.imread(filepath)\n",
    "    section = int(file_name.split('.')[0])\n",
    "    section_offset[section] = (aligned_shape - img.shape[:2][::-1]) // 2\n",
    "\n",
    "\n",
    "##### Reproduce create_alignment transform\n",
    "CLEANED = os.path.join(fileLocationManager.prep, 'CH1', 'thumbnail_cleaned')\n",
    "\n",
    "image_name_list = sorted(os.listdir(CLEANED))\n",
    "anchor_idx = len(image_name_list) // 2\n",
    "transformation_to_previous_sec = {}\n",
    "\n",
    "for i in range(1, len(image_name_list)):\n",
    "    fixed_fn = os.path.splitext(image_name_list[i - 1])[0]\n",
    "    moving_fn = os.path.splitext(image_name_list[i])[0]\n",
    "    transformation_to_previous_sec[i] = load_consecutive_section_transform(animal, moving_fn, fixed_fn)\n",
    "\n",
    "transformation_to_anchor_sec = {}\n",
    "# Converts every transformation\n",
    "for moving_idx in range(len(image_name_list)):\n",
    "    if moving_idx == anchor_idx:\n",
    "        transformation_to_anchor_sec[image_name_list[moving_idx]] = np.eye(3)\n",
    "    elif moving_idx < anchor_idx:\n",
    "        T_composed = np.eye(3)\n",
    "        for i in range(anchor_idx, moving_idx, -1):\n",
    "            T_composed = np.dot(np.linalg.inv(transformation_to_previous_sec[i]), T_composed)\n",
    "        transformation_to_anchor_sec[image_name_list[moving_idx]] = T_composed\n",
    "    else:\n",
    "        T_composed = np.eye(3)\n",
    "        for i in range(anchor_idx + 1, moving_idx + 1):\n",
    "            T_composed = np.dot(transformation_to_previous_sec[i], T_composed)\n",
    "        transformation_to_anchor_sec[image_name_list[moving_idx]] = T_composed\n",
    "\n",
    "\n",
    "warp_transforms = create_warp_transforms(animal, transformation_to_anchor_sec, downsample=True)\n",
    "ordered_transforms = sorted(warp_transforms.items())\n",
    "section_transform = {}\n",
    "\n",
    "for section, transform in ordered_transforms:\n",
    "    section_num = int(section.split('.')[0])\n",
    "    transform = np.linalg.inv(transform)\n",
    "    section_transform[section_num] = transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Alignment of annotation coordinates\n",
    "# Note, for missing sections, neuroglancer is minus one the section retrieved here\n",
    "# Litao, check this code, most of it is yours but i added the defaultdict to fill up the points\n",
    "keys = [k for k in structure_dict.keys()]\n",
    "#Litao, this missing_sections will need to be manually built up from Beth's spreadhsheet\n",
    "missing_sections = {k:[117] for k in keys}\n",
    "fill_sections = defaultdict(dict)\n",
    "other_structures = set()\n",
    "volume = np.zeros((aligned_shape[1], aligned_shape[0], num_section), dtype=np.uint8)\n",
    "for section in section_structure_vertices:\n",
    "    template = np.zeros((aligned_shape[1], aligned_shape[0]), dtype=np.uint8)\n",
    "    for structure in section_structure_vertices[section]:\n",
    "        points = np.array(section_structure_vertices[section][structure])\n",
    "        points = points / 32\n",
    "        points = points + section_offset[section]  # create_clean offset\n",
    "        points = points.reshape(1,2)\n",
    "        points = transform_create_alignment(points, section_transform[section])  # create_alignment transform\n",
    "        points = points.astype(np.int32)\n",
    "        \n",
    "        try:\n",
    "            missing_list = missing_sections[structure]\n",
    "        except:\n",
    "            missing_list = []\n",
    "            \n",
    "        if section in missing_list:\n",
    "            fill_sections[structure][section] = points\n",
    "            \n",
    "        try:\n",
    "            #color = colors[structure.upper()]\n",
    "            color = structure_dict[structure][1] # structure dict returns a list of [description, color]\n",
    "            # for each key\n",
    "        except:\n",
    "            color = 255\n",
    "            other_structures.add(structure)\n",
    "\n",
    "        cv2.polylines(template, [points], True, color, 2, lineType=cv2.LINE_AA)\n",
    "    volume[:, :, section - 1] = template\n",
    "\n",
    "# fill up missing sections\n",
    "template = np.zeros((aligned_shape[1], aligned_shape[0]), dtype=np.uint8)\n",
    "for structure,v in fill_sections.items():\n",
    "    color = structure_dict[structure][1]\n",
    "    for section, points in v.items():\n",
    "        cv2.polylines(template, [points], True, color, 2, lineType=cv2.LINE_AA)\n",
    "        volume[:, :, section] = template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel resolution in nanometer (how much nanometer each element in numpy array represent)\n",
    "resol = (14464, 14464, 20000)\n",
    "# Voxel offset\n",
    "offset = (0, 0, 0)\n",
    "# Layer type\n",
    "layer_type = 'segmentation'\n",
    "# number of channels \n",
    "num_channels = 1\n",
    "# segmentation properties in the format of [(number1, label1), (number2, label2) ...]\n",
    "# where number is an integer that is in the volume and label is a string that describes that segmenetation\n",
    "\n",
    "segmentation_properties = [(number, f'{structure}: {label}') for structure, (label, number) in structure_dict.items()]\n",
    "extra_structures = ['Pr5', 'VTg', 'DRD', 'IF', 'MPB', 'Op', 'RPC', 'LSO', 'MVe', 'CnF', \n",
    "                    'pc', 'DTgC', 'LPB', 'Pr5DM', 'DTgP', 'RMC', 'VTA', 'IPC', 'DRI', 'LDTg', \n",
    "                    'IPA', 'PTg', 'DTg', 'IPL', 'SuVe', 'Sol', 'IPR', '8n', 'Dk', 'IO', \n",
    "                    'Cb', 'Pr5VL', 'APT', 'Gr', 'RR', 'InC', 'X', 'EW']\n",
    "segmentation_properties += [(len(structure_dict) + index + 1, structure) for index, structure in enumerate(extra_structures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/seung-lab/cloud-volume\n",
    "# Don't change all compress=False. It seems that Neuroglancer can only read with compress=False\n",
    "from cloudvolume import CloudVolume\n",
    "precompute_path = f'/net/birdstore/Active_Atlas_Data/data_root/atlas_data/foundation_brain_annotations/{animal}'\n",
    "cloudpath = f'file://{precompute_path}'\n",
    "info = CloudVolume.create_new_info(\n",
    "    num_channels = num_channels,\n",
    "    layer_type = layer_type,\n",
    "    data_type = str(volume.dtype), # Channel images might be 'uint8'\n",
    "    encoding = 'raw', # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "    resolution = resol, # Voxel scaling, units are in nanometers\n",
    "    voxel_offset = offset, # x,y,z offset in voxels from the origin\n",
    "    chunk_size = [64, 64, 64], # units are voxels\n",
    "    volume_size = volume.shape, # e.g. a cubic millimeter dataset\n",
    ")\n",
    "vol = CloudVolume(cloudpath, mip=0, info=info, compress=False)\n",
    "vol.commit_info()\n",
    "vol[:, :, :] = volume[:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.info['segment_properties'] = 'names'\n",
    "vol.commit_info()\n",
    "\n",
    "segment_properties_path = os.path.join(precompute_path, 'names')\n",
    "os.makedirs(segment_properties_path, exist_ok=True)\n",
    "\n",
    "info = {\n",
    "    \"@type\": \"neuroglancer_segment_properties\", \n",
    "    \"inline\": {\n",
    "        \"ids\": [str(number) for number, label in segmentation_properties],\n",
    "        \"properties\": [{\n",
    "            \"id\": \"label\", \n",
    "            \"description\": \"Name of structures\",\n",
    "            \"type\": \"label\",\n",
    "            \"values\": [str(label) for number, label in segmentation_properties]\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(segment_properties_path, 'info'), 'w') as file:\n",
    "    json.dump(info, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/seung-lab/igneous\n",
    "tq = LocalTaskQueue(parallel=True)\n",
    "tasks = tc.create_downsampling_tasks(cloudpath, compress=False) # Downsample the volumes \n",
    "tq.insert(tasks)\n",
    "tq.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = np.swapaxes(volume, 0, 1)\n",
    "all_volume_layer = neuroglancer.SegmentationLayer(\n",
    "    source = neuroglancer.LocalVolume(\n",
    "        data=volume, \n",
    "        dimensions=neuroglancer.CoordinateSpace(names=['x', 'y', 'z'], units='nm', scales=[14464, 14464, 20000]), \n",
    "        voxel_offset=(0, 0, 0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    s.layers.clear()\n",
    "    s.layers['all'] = all_volume_layer\n",
    "print(viewer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
